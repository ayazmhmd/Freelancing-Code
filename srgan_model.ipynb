{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23942,"sourceType":"datasetVersion","datasetId":17839}],"dockerImageVersionId":30008,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["## II. Imports and globals"],"metadata":{"id":"0ADFSd2MGL_A"}},{"cell_type":"code","source":["# install an older version of tensorflow\n","# (the implementation may not work with the most recent TensorFlow release)\n","\n","!pip install keras==2.3.1\n","!pip install tensorflow==2.1.0"],"metadata":{"trusted":true,"id":"0ZGnrBzjGL_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import glob\n","import os\n","\n","from keras import Input\n","from keras.applications import VGG19\n","from keras.callbacks import TensorBoard\n","from keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense\n","from keras.layers import Conv2D, UpSampling2D\n","from keras.models import Model\n","from keras.optimizers import Adam\n","\n","\n","import random\n","from numpy import asarray\n","from itertools import repeat\n","\n","import imageio\n","from imageio import imread\n","from PIL import Image\n","from skimage.transform import resize as imresize\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","print(\"Tensorflow version \" + tf.__version__)\n","print(\"Keras version \" + tf.keras.__version__)"],"metadata":{"trusted":true,"id":"Fn98LTu9GL_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data path\n","TRAIN_PATH = r'../input/kermany2018/OCT2017 /train/'\n","VAL_PATH = r'../input/kermany2018/OCT2017 /val/'\n","TEST_PATH = r'../input/kermany2018/OCT2017 /test/'\n","data_path = TRAIN_PATH\n","\n","epochs = 5001\n","\n","# batch size equals to 8 (due to RAM limits)\n","batch_size = 8\n","\n","# define the shape of low resolution image (LR)\n","low_resolution_shape = (64, 64, 3)\n","\n","# define the shape of high resolution image (HR)\n","high_resolution_shape = (256, 256, 3)\n","\n","# optimizer for discriminator, generator\n","common_optimizer = Adam(0.0002, 0.5)\n","\n","# use seed for reproducible results\n","SEED = 2020\n","tf.random.set_seed(SEED)"],"metadata":{"trusted":true,"id":"UJh2SWTvGL_B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## III. Data\n","\n","Load data, process data, EDA"],"metadata":{"id":"erkpDfJmGL_C"}},{"cell_type":"code","source":["def get_train_images(data_path):\n","\n","    CLASSES = ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n","    image_list = []\n","\n","    for class_type in CLASSES:\n","        image_list.extend(glob.glob(data_path + class_type + '/*'))\n","\n","    return image_list"],"metadata":{"trusted":true,"id":"Ebla-A6YGL_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_img_dims(image_list):\n","\n","    min_size = []\n","    max_size = []\n","\n","    for i in range(len(image_list)):\n","        im = Image.open(image_list[i])\n","        min_size.append(min(im.size))\n","        max_size.append(max(im.size))\n","\n","    return min(min_size), max(max_size)"],"metadata":{"trusted":true,"id":"SjcUnQl8GL_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get min/max image sizes\n","\n","image_list = get_train_images(data_path)\n","min_size, max_size = find_img_dims(image_list)\n","print('The min and max image dims are {} and {} respectively.'\n","      .format(min_size, max_size))"],"metadata":{"trusted":true,"id":"_OKKt2hBGL_D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## IV. Utility functions\n","\n","Quantitative metrics for image quality  \n","Loss functions  \n","Plots  \n","Image processing: sampling and saving images"],"metadata":{"id":"zZNcNBI7GL_D"}},{"cell_type":"markdown","source":["### IV A. Metrics"],"metadata":{"id":"R08K0I1LGL_D"}},{"cell_type":"markdown","source":["#### 1. PSNR - Peak Signal-to-Noise ratio\n","\n","\n","PSNR is the ratio between maximum possible power of signal and power of corrupting noise (Wikipedia).\n","\n","\n","$${ PSNR = 10  \\log_{10}  \\left( {MAX_I^2 \\over MSE} \\right) }$$\n","\n","$ MAX_I $  -  maximum possible power of a signal of image I  \n","$ MSE $  -  mean squared error pixel by pixel"],"metadata":{"id":"wNLrbQ8eGL_E"}},{"cell_type":"code","source":["def compute_psnr(original_image, generated_image):\n","\n","    original_image = tf.convert_to_tensor(original_image, dtype=tf.float32)\n","    generated_image = tf.convert_to_tensor(generated_image, dtype=tf.float32)\n","    psnr = tf.image.psnr(original_image, generated_image, max_val=1.0)\n","\n","    return tf.math.reduce_mean(psnr, axis=None, keepdims=False, name=None)"],"metadata":{"trusted":true,"id":"OZ8UY3ghGL_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_psnr(psnr):\n","\n","    psnr_means = psnr['psnr_quality']\n","    plt.figure(figsize=(10,8))\n","    plt.plot(psnr_means)\n","    plt.xlabel('Epochs')\n","    plt.ylabel('PSNR')\n","    plt.title('PSNR')"],"metadata":{"trusted":true,"id":"GRk71xDNGL_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. SSIM - Structural Similarity Index\n","\n","\n","SSIM measures the perceptual difference between two similar images [(see Wikipedia)](https://en.wikipedia.org/wiki/Structural_similarity).\n","\n","$${ SSIM(x, y) = {(2 \\mu_x \\mu_y + c_1) (2 \\sigma_{xy} + c_2) \\over (\\mu_x^2 + \\mu_y^2 + c_1) ( \\sigma_x^2 + \\sigma_y^2 +c_2)}  }$$\n","\n","\n","$ \\mu_x, \\mu_y$       - average value for image $x, y$    \n","$ \\sigma_x, \\sigma_y$ - standard deviation for image $x, y$     \n","$ \\sigma_{xy}$        - covariance  of $x$ and $y$      \n","$ c_1, c_2 $          - coefficients"],"metadata":{"id":"GpxnCpmyGL_E"}},{"cell_type":"code","source":["def compute_ssim(original_image, generated_image):\n","\n","    original_image = tf.convert_to_tensor(original_image, dtype=tf.float32)\n","    generated_image = tf.convert_to_tensor(generated_image, dtype=tf.float32)\n","    ssim = tf.image.ssim(original_image, generated_image, max_val=1.0, filter_size=11,\n","                          filter_sigma=1.5, k1=0.01, k2=0.03)\n","\n","    return tf.math.reduce_mean(ssim, axis=None, keepdims=False, name=None)"],"metadata":{"trusted":true,"id":"EA7Pc3wWGL_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_ssim(ssim):\n","\n","    ssim_means = ssim['ssim_quality']\n","\n","    plt.figure(figsize=(10,8))\n","    plt.plot(ssim_means)\n","    plt.xlabel('Epochs')\n","    plt.ylabel('SSIM')\n","    plt.title('SSIM')"],"metadata":{"trusted":true,"id":"HfzpIYjKGL_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IV B. Loss Functions\n","\n","The most important contribution of the SRGAN paper was the use of a *perceptual loss* function.\n","\n","\n","**Perceptual Loss**  is a weighted sum of the *content loss* and *adversarial loss*.\n","\n","\n","$${ l^{SR} = l_X^{SR} + 10^{-3}l_{Gen}^{SR}}$$\n","\n","$l^{SR}$ - perceptual loss   \n","$l_X^{SR}$ - content loss   \n","$l_{Gen}^{SR}$ - adversarial loss\n","\n","\n","****************************\n","\n","**1. Content Loss**   \n","The SRGAN replaced the *MSE loss* with a *VGG loss*. Both losses are defined below:\n","\n","         \n","**Pixel-wise MSE loss** is the mean squared error between each pixel in the original HR image and a the corresponding pixel in the generated SR image.\n","\n","\n","**VGG loss** is the euclidean distance between the feature maps of the generated SR image and the original HR  image. The feature maps are the activation layers of the pre-trained  VGG 19 network.\n","\n","$${ l_{{VGG}/{i,j}}^{SR} = {1 \\over {W_{i,j}H_{i,j}}} \\sum\\limits_{x=1}^{W_{i,j}} \\sum\\limits_{y=1}^{H_{i,j}}  ({\\phi}_{i,j}(I^{HR})_{x,y} - {\\phi}_{i,j} (G_{{\\theta}_G} (I^{LR}))_{x,y})^2}$$\n","\n","\n","$ l_{{VGG}/{i,j}}^{SR} $  -  VGG loss    \n","$ {\\phi}_{i,j} $  -   the feature map obtained by the j-th convolution (after activation) before the i-th maxpooling layer within the VGG19 network\n","\n","\n","\n","**2. Adversarial Loss**  \n","This is calculated based on probabilities provided by Discriminator.\n","\n","$${ l_{Gen}^{SR} = \\sum\\limits_{n=1}^{N} - \\log{D_{{\\theta}_D}} (G_{{\\theta}_G} (I^{LR}))}$$\n","\n","$ l_{Gen}^{SR} $  -  generative loss  \n","$ D $  -  discriminator function    \n","$ D_{{\\theta}_D} $  -  discriminator function parametrized with $ {\\theta}_D $   \n","$ {D_{{\\theta}_D}} (G_{{\\theta}_G} (I^{LR})) $   -  probability that the reconstructed image $\n","$ G_{{\\theta}_G} (I^{LR}) $  is a natural HR image"],"metadata":{"id":"rwalySCRGL_E"}},{"cell_type":"markdown","source":["#### Plot loss function"],"metadata":{"id":"gZYApMcMGL_F"}},{"cell_type":"code","source":["def plot_loss(losses):\n","\n","    d_loss = losses['d_history']\n","    g_loss = losses['g_history']\n","\n","\n","    plt.figure(figsize=(10,8))\n","    plt.plot(d_loss, label=\"Discriminator loss\")\n","    plt.plot(g_loss, label=\"Generator loss\")\n","\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title(\"Loss\")\n","    plt.legend()"],"metadata":{"trusted":true,"id":"-ujZiDMkGL_F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IV C. Sampling, saving images"],"metadata":{"id":"DScAEaLOGL_F"}},{"cell_type":"code","source":["def sample_images(image_list, batch_size, high_resolution_shape, low_resolution_shape):\n","\n","    \"\"\"\n","    Pre-process a batch of training images\n","    \"\"\"\n","\n","    # image_list is the list of all images\n","    # ransom sample a batch of images\n","    images_batch = np.random.choice(image_list, size=batch_size)\n","\n","    lr_images = []\n","    hr_images = []\n","\n","\n","    for img in images_batch:\n","\n","        img1 = imread(img, as_gray=False, pilmode='RGB')\n","        #img1 = imread(img, pilmode='RGB')\n","        img1 = img1.astype(np.float32)\n","\n","        # change the size\n","        img1_high_resolution = imresize(img1, high_resolution_shape)\n","        img1_low_resolution = imresize(img1, low_resolution_shape)\n","\n","\n","        # do a random horizontal flip\n","        if np.random.random() < 0.5:\n","            img1_high_resolution = np.fliplr(img1_high_resolution)\n","            img1_low_resolution = np.fliplr(img1_low_resolution)\n","\n","        hr_images.append(img1_high_resolution)\n","        lr_images.append(img1_low_resolution)\n","\n","\n","    # convert lists into numpy ndarrays\n","    return np.array(hr_images), np.array(lr_images)"],"metadata":{"trusted":true,"id":"JDcXkDnVGL_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_images(original_image, lr_image, sr_image, path):\n","\n","    \"\"\"\n","    Save LR, HR (original) and generated SR\n","    images in one panel\n","    \"\"\"\n","\n","    fig, ax = plt.subplots(1,3, figsize=(10, 6))\n","\n","    images = [original_image, lr_image, sr_image]\n","    titles = ['HR', 'LR','SR - generated']\n","\n","    for idx,img in enumerate(images):\n","        # (X + 1)/2 to scale back from [-1,1] to [0,1]\n","        ax[idx].imshow((img + 1)/2.0, cmap='gray')\n","        ax[idx].axis(\"off\")\n","    for idx, title in enumerate(titles):\n","        ax[idx].set_title('{}'.format(title))\n","\n","    plt.savefig(path)"],"metadata":{"trusted":true,"id":"edjZ-KHOGL_F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## V. SRGAN-VGG19"],"metadata":{"id":"NofMfGUFGL_F"}},{"cell_type":"markdown","source":["The SRGAN has the following code components:\n"," 1. Generator network\n"," 2.  Discriminator network\n"," 3. Feature extractor using the VGG19 network\n"," 4. Adversarial framework"],"metadata":{"id":"S-DBWL8YGL_G"}},{"cell_type":"markdown","source":["### V 1. Generator"],"metadata":{"id":"rI6jZ36hGL_G"}},{"cell_type":"markdown","source":["There are 16 residual blocks and 2 upsampling blocks. The generator follows the architecture outlined in [2]."],"metadata":{"id":"5auuYg9vGL_G"}},{"cell_type":"code","source":["def residual_block(x):\n","\n","    filters = [64, 64]\n","    kernel_size = 3\n","    strides = 1\n","    padding = \"same\"\n","    momentum = 0.8\n","    activation = \"relu\"\n","\n","    res = Conv2D(filters=filters[0], kernel_size=kernel_size, strides=strides, padding=padding)(x)\n","    res = Activation(activation=activation)(res)\n","    res = BatchNormalization(momentum=momentum)(res)\n","\n","    res = Conv2D(filters=filters[1], kernel_size=kernel_size, strides=strides, padding=padding)(res)\n","    res = BatchNormalization(momentum=momentum)(res)\n","\n","    res = Add()([res, x])\n","\n","    return res"],"metadata":{"trusted":true,"id":"6NB6Rds2GL_G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_generator():\n","\n","    # use 16 residual blocks in generator\n","    residual_blocks = 16\n","    momentum = 0.8\n","\n","    # input LR dimension: 4x downsample of HR\n","    input_shape = (64, 64, 3)\n","\n","    # input for the generator\n","    input_layer = Input(shape=input_shape)\n","\n","    # pre-residual block: conv layer before residual blocks\n","    gen1 = Conv2D(filters=64, kernel_size=9, strides=1, padding='same', activation='relu')(input_layer)\n","\n","    # add 16 residual blocks\n","    res = residual_block(gen1)\n","    for i in range(residual_blocks - 1):\n","        res = residual_block(res)\n","\n","    # post-residual block: conv and batch-norm layer after residual blocks\n","    gen2 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(res)\n","    gen2 = BatchNormalization(momentum=momentum)(gen2)\n","\n","    # take the sum of pre-residual block(gen1) and post-residual block(gen2)\n","    gen3 = Add()([gen2, gen1])\n","\n","    # upsampling\n","    gen4 = UpSampling2D(size=2)(gen3)\n","    gen4 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen4)\n","    gen4 = Activation('relu')(gen4)\n","\n","    # upsampling\n","    gen5 = UpSampling2D(size=2)(gen4)\n","    gen5 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen5)\n","    gen5 = Activation('relu')(gen5)\n","\n","    # conv layer at the output\n","    gen6 = Conv2D(filters=3, kernel_size=9, strides=1, padding='same')(gen5)\n","    output = Activation('tanh')(gen6)\n","\n","    # model\n","    model = Model(inputs=[input_layer], outputs=[output], name='generator')\n","\n","    return model"],"metadata":{"trusted":true,"id":"-j69ib3gGL_G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = build_generator()"],"metadata":{"trusted":true,"id":"ozb6FULJGL_G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### V 2. Discriminator"],"metadata":{"id":"7JCQ-iVrGL_G"}},{"cell_type":"code","source":["def build_discriminator():\n","\n","    # define hyperparameters\n","    leakyrelu_alpha = 0.2\n","    momentum = 0.8\n","\n","    # the input is the HR shape\n","    input_shape = (256, 256, 3)\n","\n","    # input layer for discriminator\n","    input_layer = Input(shape=input_shape)\n","\n","    # 8 convolutional layers with batch normalization\n","    dis1 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(input_layer)\n","    dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n","\n","    dis2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(dis1)\n","    dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n","    dis2 = BatchNormalization(momentum=momentum)(dis2)\n","\n","    dis3 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same')(dis2)\n","    dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n","    dis3 = BatchNormalization(momentum=momentum)(dis3)\n","\n","    dis4 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis3)\n","    dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n","    dis4 = BatchNormalization(momentum=0.8)(dis4)\n","\n","    dis5 = Conv2D(256, kernel_size=3, strides=1, padding='same')(dis4)\n","    dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n","    dis5 = BatchNormalization(momentum=momentum)(dis5)\n","\n","    dis6 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(dis5)\n","    dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n","    dis6 = BatchNormalization(momentum=momentum)(dis6)\n","\n","    dis7 = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(dis6)\n","    dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n","    dis7 = BatchNormalization(momentum=momentum)(dis7)\n","\n","    dis8 = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(dis7)\n","    dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n","    dis8 = BatchNormalization(momentum=momentum)(dis8)\n","\n","    # fully connected layer\n","    dis9 = Dense(units=1024)(dis8)\n","    dis9 = LeakyReLU(alpha=0.2)(dis9)\n","\n","    # last fully connected layer - for classification\n","    output = Dense(units=1, activation='sigmoid')(dis9)\n","\n","    model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n","\n","    return model"],"metadata":{"trusted":true,"id":"mdDNJ7mNGL_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["discriminator = build_discriminator()\n","discriminator.trainable = True\n","discriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])"],"metadata":{"trusted":true,"id":"47BXqeATGL_H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### V 3. VGG19 Feature Extractor"],"metadata":{"id":"lXW9nlpFGL_H"}},{"cell_type":"code","source":["VGG19_base = VGG19(weights=\"imagenet\")"],"metadata":{"trusted":true,"id":"6Sf_Wwc9GL_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_VGG19():\n","\n","    input_shape = (256, 256, 3)\n","    VGG19_base.outputs = [VGG19_base.get_layer('block5_conv2').output]\n","    input_layer = Input(shape=input_shape)\n","    features = VGG19_base(input_layer)\n","    model = Model(inputs=[input_layer], outputs=[features])\n","\n","    return model"],"metadata":{"trusted":true,"id":"oltDUjm8GL_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fe_model = build_VGG19()\n","fe_model.trainable = False\n","fe_model.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])"],"metadata":{"trusted":true,"id":"lsm1mbmcGL_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_adversarial_model(generator, discriminator, feature_extractor):\n","\n","    # input layer for high-resolution images\n","    input_high_resolution = Input(shape=high_resolution_shape)\n","\n","    # input layer for low-resolution images\n","    input_low_resolution = Input(shape=low_resolution_shape)\n","\n","    # generate high-resolution images from low-resolution images\n","    generated_high_resolution_images = generator(input_low_resolution)\n","\n","    # extract feature maps from generated images\n","    features = feature_extractor(generated_high_resolution_images)\n","\n","    # make a discriminator non-trainable\n","    discriminator.trainable = False\n","    discriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n","\n","    # discriminator will give us a probability estimation for the generated high-resolution images\n","    probs = discriminator(generated_high_resolution_images)\n","\n","    # create and compile\n","    adversarial_model = Model([input_low_resolution, input_high_resolution], [probs, features])\n","    adversarial_model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1e-3, 1], optimizer=common_optimizer)\n","\n","    return adversarial_model\n"],"metadata":{"trusted":true,"id":"GAvwp1o6GL_M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["adversarial_model = build_adversarial_model(generator, discriminator, fe_model)"],"metadata":{"trusted":true,"id":"3SBwA90pGL_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## VI. Training\n"],"metadata":{"id":"0e9MuVr0GL_M"}},{"cell_type":"code","source":["# initialize\n","\n","losses = {\"d_history\":[], \"g_history\":[]}\n","psnr = {'psnr_quality': []}\n","ssim = {'ssim_quality': []}"],"metadata":{"trusted":true,"id":"b7mAjjh1GL_M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training loop\n","\n","for epoch in range(epochs):\n","\n","    d_history = []\n","    g_history = []\n","\n","    image_list = get_train_images(data_path)\n","\n","    \"\"\"\n","    Train the discriminator network\n","    \"\"\"\n","\n","    hr_images, lr_images = sample_images(image_list,\n","                                         batch_size=batch_size,\n","                                         low_resolution_shape=low_resolution_shape,\n","                                         high_resolution_shape=high_resolution_shape)\n","\n","\n","    # normalize the images\n","    hr_images = hr_images / 127.5 - 1.\n","    lr_images = lr_images / 127.5 - 1.\n","\n","    # generate high-resolution images from low-resolution images\n","    generated_high_resolution_images = generator.predict(lr_images)\n","\n","    # generate a batch of true and fake labels\n","    real_labels = np.ones((batch_size, 16, 16, 1))\n","    fake_labels = np.zeros((batch_size, 16, 16, 1))\n","\n","\n","    d_loss_real = discriminator.train_on_batch(hr_images, real_labels)\n","    d_loss_real =  np.mean(d_loss_real)\n","    d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake_labels)\n","    d_loss_fake =  np.mean(d_loss_fake)\n","\n","    # calculate total loss of discriminator as average loss on true and fake labels\n","    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","    losses['d_history'].append(d_loss)\n","\n","\n","    \"\"\"\n","        Train the generator network\n","    \"\"\"\n","\n","    # sample a batch of images\n","    hr_images, lr_images = sample_images(image_list,\n","                                         batch_size=batch_size,\n","                                         low_resolution_shape=low_resolution_shape,\n","                                         high_resolution_shape=high_resolution_shape)\n","\n","\n","    # normalize the images\n","    hr_images = hr_images / 127.5 - 1.\n","    lr_images = lr_images / 127.5 - 1.\n","\n","\n","\n","    # extract feature maps for true high-resolution images\n","    image_features = fe_model.predict(hr_images)\n","\n","\n","\n","    # train the generator\n","    g_loss = adversarial_model.train_on_batch([lr_images, hr_images],\n","                                               [real_labels, image_features])\n","\n","    losses['g_history'].append(0.5 * (g_loss[1]))\n","\n","\n","\n","    # calculate the psnr\n","    ps = compute_psnr(hr_images, generated_high_resolution_images)\n","    psnr['psnr_quality'].append(ps)\n","\n","    # calculate the ssim\n","    ss = compute_ssim(hr_images, generated_high_resolution_images)\n","    ssim['ssim_quality'].append(ss)\n","\n","\n","\n","    \"\"\"\n","        save and print image samples\n","    \"\"\"\n","\n","    if epoch % 500 == 0:\n","\n","        hr_images, lr_images = sample_images(image_list,\n","                                             batch_size=batch_size,\n","                                             low_resolution_shape=low_resolution_shape,\n","                                             high_resolution_shape=high_resolution_shape)\n","\n","\n","        # normalize the images\n","        hr_images = hr_images / 127.5 - 1.\n","        lr_images = lr_images / 127.5 - 1.\n","\n","\n","        generated_images = generator.predict_on_batch(lr_images)\n","\n","        for index, img in enumerate(generated_images):\n","            if index < 3:   # comment this line to display all the images\n","                save_images(hr_images[index], lr_images[index], img,\n","                            path=\"/kaggle/working/img_{}_{}\".format(epoch, index))"],"metadata":{"trusted":true,"id":"Qswxo-e6GL_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plots - post training\n","\n","plot_loss(losses)\n","plot_psnr(psnr)\n","plot_ssim(ssim)"],"metadata":{"trusted":true,"id":"K2AMP5QpGL_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save model weights\n","\n","generator.save_weights(\"/kaggle/working/srgan_generator.h5\")\n","discriminator.save_weights(\"/kaggle/working/srgan_discriminator.h5\")"],"metadata":{"trusted":true,"id":"UQJPcHbCGL_N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## VII. References and further reading\n","\n","\n","<a name=\"ref1\"></a>[1] [Goodfellow et al. 'Generative Adversarial Nets'](https://arxiv.org/pdf/1406.2661.pdf)\n","\n","<a name=\"ref2\"></a>[2] [Ledig et al. 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network'](https://arxiv.org/abs/1609.04802)\n","\n","<a name=\"ref3\"></a>[3] [Kailash Ahirwar. 'Generative Adversarial Networks Projects'](https://github.com/PacktPublishing/Generative-Adversarial-Networks-Projects)\n","\n","<a name=\"ref4\"></a>[4] [Saeed Anwar et al. 'A Deep Journey into Super-resolution: A Survey'](https://arxiv.org/pdf/1904.07523.pdf)\n","\n","<a name=\"ref5\"></a>[5] [Xintao Wang et al. 'ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks'](https://arxiv.org/pdf/1809.00219.pdf)"],"metadata":{"id":"51zGSzqEGL_N"}}]}