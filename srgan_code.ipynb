{"cells":[{"cell_type":"markdown","metadata":{"id":"ZDihzTidIVRP"},"source":["### Libraries üìö‚¨á"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26440,"status":"ok","timestamp":1720442175532,"user":{"displayName":"Gan Project","userId":"10714615017816106068"},"user_tz":-300},"id":"uVk_-7OaYPNy","outputId":"9a57084d-4a78-4d2f-cec3-0f716160b455"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"bCvjPeIcIVRQ","executionInfo":{"status":"ok","timestamp":1720442182921,"user_tz":-300,"elapsed":7399,"user":{"displayName":"Gan Project","userId":"10714615017816106068"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os, math, sys\n","import glob, itertools\n","import argparse, random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torchvision.models import vgg19\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.utils import save_image, make_grid\n","\n","import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","from tqdm import tqdm_notebook as tqdm\n","\n","random.seed(42)\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_NES5UDdb1vS","executionInfo":{"status":"ok","timestamp":1720442182922,"user_tz":-300,"elapsed":22,"user":{"displayName":"Gan Project","userId":"10714615017816106068"}}},"outputs":[],"source":["# load pretrained models\n","load_pretrained_models = False\n","# number of epochs of training\n","n_epochs = 20\n","# size of the batches\n","batch_size = 16\n","# adam: learning rate\n","lr = 0.00008\n","# adam: decay of first order momentum of gradient\n","b1 = 0.5\n","# adam: decay of second order momentum of gradient\n","b2 = 0.999\n","# epoch from which to start lr decay\n","decay_epoch = 100\n","# number of cpu threads to use during batch generation\n","n_cpu = 8\n","# high res. image height\n","hr_height = 512\n","# high res. image width\n","hr_width = 512\n","# number of image channels\n","channels = 3\n","\n","os.makedirs(\"images\", exist_ok=True)\n","os.makedirs(\"saved_models\", exist_ok=True)\n","\n","cuda = torch.cuda.is_available()\n","hr_shape = (hr_height, hr_width)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Hrd2uVU1YZVK","executionInfo":{"status":"ok","timestamp":1720442182922,"user_tz":-300,"elapsed":19,"user":{"displayName":"Gan Project","userId":"10714615017816106068"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/project')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1720442182922,"user":{"displayName":"Gan Project","userId":"10714615017816106068"},"user_tz":-300},"id":"C6vMdlrAYcZ6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ccd0b3c-7662-4687-c7c3-27f79a134ee4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Mobile_crane',\n"," '.ipynb_checkpoints',\n"," 'Tower_crane',\n"," 'low_Mobile_crane',\n"," 'low_Tower_crane']"]},"metadata":{},"execution_count":5}],"source":["os.listdir('/content/drive/MyDrive/project/7-Detection data')"]},{"cell_type":"markdown","metadata":{"id":"zHFyr7xkIVRS"},"source":["### Settings ‚öôÔ∏è"]},{"cell_type":"markdown","metadata":{"id":"HDNp8MbyIVRV"},"source":["### Define Dataset Class"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"4frxKy85IVRW","executionInfo":{"status":"ok","timestamp":1720442184489,"user_tz":-300,"elapsed":1580,"user":{"displayName":"Gan Project","userId":"10714615017816106068"}}},"outputs":[],"source":["import glob\n","import numpy as np\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","# # Normalization parameters\n","# mean = np.array([0.485, 0.456, 0.406])\n","# std = np.array([0.229, 0.224, 0.225])\n","\n","class ImageDataset(Dataset):\n","    def __init__(self, low_paths, high_paths):\n","        # Define transforms for images\n","        self.transform_lr = transforms.Compose([\n","            transforms.Resize((64, 64)),\n","            transforms.ToTensor()\n","\n","        ])\n","\n","        self.transform_hr = transforms.Compose([\n","            transforms.Resize((512, 512)),\n","            transforms.ToTensor()\n","        ])\n","\n","        self.low_paths = low_paths\n","        self.high_paths = high_paths\n","\n","    def __getitem__(self, index):\n","        img_low = Image.open(self.low_paths[index % len(self.low_paths)])\n","        img_high = Image.open(self.high_paths[index % len(self.high_paths)])\n","\n","        img_lr = self.transform_lr(img_low)\n","        img_hr = self.transform_hr(img_high)\n","\n","        return {\"lr\": img_lr, \"hr\": img_hr}\n","\n","    def __len__(self):\n","        return max(len(self.low_paths), len(self.high_paths))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":8120,"status":"ok","timestamp":1720442192604,"user":{"displayName":"Gan Project","userId":"10714615017816106068"},"user_tz":-300},"id":"hZQacs5_aUNR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"22496eb6-c8ca-4af8-ec07-1a1c4904ca7a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2009"]},"metadata":{},"execution_count":7}],"source":["len(os.listdir('/content/drive/MyDrive/project/7-Detection data/Mobile_crane'))"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1831,"status":"ok","timestamp":1720442194421,"user":{"displayName":"Gan Project","userId":"10714615017816106068"},"user_tz":-300},"id":"tdnShXdIaXvh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"06b182d5-a9f8-43ba-d552-c660438484cd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2022"]},"metadata":{},"execution_count":8}],"source":["len(os.listdir('/content/drive/MyDrive/project/7-Detection data/Tower_crane'))"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":595,"status":"ok","timestamp":1720442249160,"user":{"displayName":"Gan Project","userId":"10714615017816106068"},"user_tz":-300},"id":"EuMe5upCZXmK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"524c8e52-595d-423f-b3d8-1ad07e2f4203"},"outputs":[{"output_type":"stream","name":"stdout","text":["4031\n","4031\n"]}],"source":["import glob\n","\n","dataset_path = '7-Detection data'\n","\n","low_paths1 = glob.glob(dataset_path + \"/low_Mobile_crane/*.*\")\n","low_paths2 = glob.glob(dataset_path + \"/low_Tower_crane/*.*\")\n","high_paths1 = glob.glob(dataset_path + \"/Mobile_crane/*.*\")\n","high_paths2 = glob.glob(dataset_path + \"/Tower_crane/*.*\")\n","\n","\n","low_paths = sorted(low_paths1 + low_paths2)\n","high_paths = sorted(high_paths1 + high_paths2)\n","\n","print(len(low_paths))\n","print(len(high_paths))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjUNsBTXidRJ"},"outputs":[],"source":["low_paths=low_paths[:400]\n","high_paths=high_paths[:400]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUq1pPN_ZrBo"},"outputs":[],"source":["train_low_paths, test_low_paths = train_test_split(low_paths, test_size=0.2, random_state=42)\n","train_high_paths, test_high_paths = train_test_split(high_paths, test_size=0.2, random_state=42)\n","\n","batch_size = 4\n","n_cpu = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1719571784562,"user":{"displayName":"Gan Project","userId":"10714615017816106068"},"user_tz":-300},"id":"SeDEr2gsbTwb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ce03fafb-614b-4aca-97ce-33a764185f52"},"outputs":[{"output_type":"stream","name":"stdout","text":["320\n","320\n"]}],"source":["print(len(train_low_paths))\n","print(len(train_high_paths))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1719571784563,"user":{"displayName":"Gan Project","userId":"10714615017816106068"},"user_tz":-300},"id":"C81fFylSbYw8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"24f6b283-7a24-4b64-9b55-f539f9b4e208"},"outputs":[{"output_type":"stream","name":"stdout","text":["80\n","80\n"]}],"source":["print(len(test_low_paths))\n","print(len(test_high_paths))"]},{"cell_type":"markdown","metadata":{"id":"QqNSVeFBIVRX"},"source":["### Get Train/Test Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-mpGG-ksZpod"},"outputs":[],"source":["train_dataset = ImageDataset(train_low_paths, train_high_paths)\n","test_dataset = ImageDataset(test_low_paths, test_high_paths)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=int(batch_size * 0.75), shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"Ic3I1zU_IVRa"},"source":["### Define Model Classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66reQiTKIVRa"},"outputs":[],"source":["# class FeatureExtractor(nn.Module):\n","#     def __init__(self):\n","#         super(FeatureExtractor, self).__init__()\n","#         vgg19_model = vgg19(pretrained=True)\n","#         self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n","\n","#     def forward(self, img):\n","#         return self.feature_extractor(img)\n","\n","\n","# class ResidualBlock(nn.Module):\n","#     def __init__(self, in_features):\n","#         super(ResidualBlock, self).__init__()\n","#         self.conv_block = nn.Sequential(\n","#             nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n","#             nn.BatchNorm2d(in_features, 0.8),\n","#             nn.PReLU(),\n","#             nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n","#             nn.BatchNorm2d(in_features, 0.8),\n","#         )\n","\n","#     def forward(self, x):\n","#         return x + self.conv_block(x)\n","\n","\n","# class GeneratorVGG(nn.Module):\n","#     def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n","#         super(GeneratorVGG, self).__init__()\n","\n","#         # First layer\n","#         self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n","\n","#         # Residual blocks\n","#         res_blocks = []\n","#         for _ in range(n_residual_blocks):\n","#             res_blocks.append(ResidualBlock(64))\n","#         self.res_blocks = nn.Sequential(*res_blocks)\n","\n","#         # Second conv layer post residual blocks\n","#         self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n","\n","#         # Upsampling layers\n","#         upsampling = []\n","#         for out_features in range(2):\n","#             upsampling += [\n","#                 # nn.Upsample(scale_factor=2),\n","#                 nn.Conv2d(64, 256, 3, 1, 1),\n","#                 nn.BatchNorm2d(256),\n","#                 nn.PixelShuffle(upscale_factor=2),\n","#                 nn.PReLU(),\n","#             ]\n","#         self.upsampling = nn.Sequential(*upsampling)\n","\n","#         # Final output layer\n","#         self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Sigmoid())\n","\n","#     def forward(self, x):\n","#         out1 = self.conv1(x)\n","#         out = self.res_blocks(out1)\n","#         out2 = self.conv2(out)\n","#         out = torch.add(out1, out2)\n","#         out = self.upsampling(out)\n","#         out = self.conv3(out)\n","#         # Clamp the output to [0, 1] range\n","#         out = torch.clamp(out, min=0.0, max=1.0)\n","#         return out\n","\n","\n","# class Discriminator(nn.Module):\n","#     def __init__(self, input_shape):\n","#         super(Discriminator, self).__init__()\n","\n","#         self.input_shape = input_shape\n","#         in_channels, in_height, in_width = self.input_shape\n","#         patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n","#         self.output_shape = (1, patch_h, patch_w)\n","\n","#         def discriminator_block(in_filters, out_filters, first_block=False):\n","#             layers = []\n","#             layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n","#             if not first_block:\n","#                 layers.append(nn.BatchNorm2d(out_filters))\n","#             layers.append(nn.LeakyReLU(0.2, inplace=True))\n","#             layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n","#             layers.append(nn.BatchNorm2d(out_filters))\n","#             layers.append(nn.LeakyReLU(0.2, inplace=True))\n","#             return layers\n","\n","#         layers = []\n","#         in_filters = in_channels\n","#         for i, out_filters in enumerate([64, 128, 256, 512]):\n","#             layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n","#             in_filters = out_filters\n","\n","#         layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n","\n","#         self.model = nn.Sequential(*layers)\n","\n","#     def forward(self, img):\n","#         return self.model(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxjdulKW0vXN"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import vgg19\n","class FeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(FeatureExtractor, self).__init__()\n","        vgg19_model = vgg19(pretrained=True)\n","        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n","\n","    def forward(self, img):\n","        return self.feature_extractor(img)\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_features):\n","        super(ResidualBlock, self).__init__()\n","        self.conv_block = nn.Sequential(\n","            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(in_features),\n","            nn.PReLU(),\n","            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(in_features),\n","        )\n","\n","    def forward(self, x):\n","        return x + self.conv_block(x)\n","\n","\n","class GeneratorVGG(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n","        super(GeneratorVGG, self).__init__()\n","\n","        # First layer\n","        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n","\n","        # Residual blocks\n","        res_blocks = []\n","        for _ in range(n_residual_blocks):\n","            res_blocks.append(ResidualBlock(64))\n","        self.res_blocks = nn.Sequential(*res_blocks)\n","\n","        # Second conv layer post residual blocks\n","        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64))\n","\n","        # Upsampling layers\n","        upsampling = []\n","        for out_features in range(3):\n","            upsampling += [\n","                nn.Conv2d(64, 256, 3, 1, 1),\n","                nn.BatchNorm2d(256),\n","                nn.PixelShuffle(upscale_factor=2),\n","                nn.PReLU(),\n","            ]\n","        self.upsampling = nn.Sequential(*upsampling)\n","\n","        # Final output layer\n","        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Sigmoid())\n","\n","    def forward(self, x):\n","        out1 = self.conv1(x)\n","        out = self.res_blocks(out1)\n","        out2 = self.conv2(out)\n","        out = torch.add(out1, out2)\n","        out = self.upsampling(out)\n","        out = self.conv3(out)\n","        # Clamp the output to [0, 1] range\n","        out = torch.clamp(out, min=0.0, max=1.0)\n","        return out\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, input_shape):\n","        super(Discriminator, self).__init__()\n","\n","        self.input_shape = input_shape\n","        in_channels, in_height, in_width = self.input_shape\n","        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n","        self.output_shape = (1, patch_h, patch_w)\n","\n","        def discriminator_block(in_filters, out_filters, first_block=False):\n","            layers = []\n","            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n","            if not first_block:\n","                layers.append(nn.BatchNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n","            layers.append(nn.BatchNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        layers = []\n","        in_filters = in_channels\n","        for i, out_filters in enumerate([64, 128, 256, 512]):\n","            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n","            in_filters = out_filters\n","\n","        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, img):\n","        return self.model(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10822,"status":"ok","timestamp":1719571795370,"user":{"displayName":"Gan Project","userId":"10714615017816106068"},"user_tz":-300},"id":"zv0sGb9-3ZEP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9273514-16af-42f5-bbae-53ba418e744f"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 548M/548M [00:05<00:00, 95.9MB/s]\n"]}],"source":["# Initialize generator and discriminator\n","generator = GeneratorVGG()\n","discriminator = Discriminator(input_shape=(channels, *hr_shape))\n","feature_extractor = FeatureExtractor()\n","\n","# Set feature extractor to inference mode\n","feature_extractor.eval()\n","\n","# Losses\n","criterion_GAN = torch.nn.MSELoss()\n","criterion_content = torch.nn.L1Loss()\n","\n","if cuda:\n","    generator = generator.cuda()\n","    discriminator = discriminator.cuda()\n","    feature_extractor = feature_extractor.cuda()\n","    criterion_GAN = criterion_GAN.cuda()\n","    criterion_content = criterion_content.cuda()\n","\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"]},{"cell_type":"markdown","metadata":{"id":"pGirQkUjIVRc"},"source":["### Train Super Resolution GAN (SRGAN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kHUpXkwKIVRc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dd6e3602-cf2d-4a25-d932-d8f49a63abb6"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training Epoch 0 :  29%|‚ñà‚ñà‚ñâ       | 23/80 [49:05<1:59:58, 126.29s/it, PSNR=10.8797, disc_loss=0.212, gen_loss=0.387]"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","from torchvision.utils import make_grid, save_image\n","from tqdm import tqdm\n","import numpy as np\n","from skimage.metrics import peak_signal_noise_ratio as PSNR\n","from skimage.metrics import structural_similarity as SSIM\n","\n","# Assuming you have defined your models, optimizers, loss functions, and data loaders somewhere\n","\n","train_gen_losses, train_disc_losses, train_psnr_values = [], [], []\n","test_gen_losses, test_disc_losses, test_psnr_values = [], [], []\n","\n","# Assuming test_counter initialization elsewhere in your code\n","\n","for epoch in range(n_epochs):\n","\n","    ### Training\n","    epoch_gen_loss, epoch_disc_loss = 0, 0\n","    epoch_psnr = 0\n","    tqdm_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch} ', total=len(train_dataloader))\n","\n","    for batch_idx, imgs in enumerate(tqdm_bar):\n","        generator.train()\n","        discriminator.train()\n","\n","        # Configure model input\n","        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n","        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n","\n","        # Adversarial ground truths\n","        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n","        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n","\n","        ### Train Generator\n","        optimizer_G.zero_grad()\n","        # Generate a high resolution image from low resolution input\n","        gen_hr = generator(imgs_lr)\n","\n","        # Adversarial loss\n","        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n","        # Content loss\n","        gen_features = feature_extractor(gen_hr)\n","        real_features = feature_extractor(imgs_hr)\n","        loss_content = criterion_content(gen_features, real_features.detach())\n","        # Total loss\n","        loss_G = loss_content + 1e-3 * loss_GAN\n","        loss_G.backward()\n","        optimizer_G.step()\n","\n","        ### Train Discriminator\n","        optimizer_D.zero_grad()\n","        # Loss of real and fake images\n","        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n","        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n","        # Total loss\n","        loss_D = (loss_real + loss_fake) / 2\n","        loss_D.backward()\n","        optimizer_D.step()\n","\n","        # Accumulate losses\n","        epoch_gen_loss += loss_G.item()\n","        epoch_disc_loss += loss_D.item()\n","\n","        # Calculate PSNR\n","        imgs_hr_np = imgs_hr.cpu().detach().numpy()\n","        gen_hr_np = gen_hr.cpu().detach().numpy()\n","        psnr_value = PSNR(imgs_hr_np, gen_hr_np, data_range=1.0)\n","        epoch_psnr += psnr_value\n","\n","        tqdm_bar.set_postfix(gen_loss=epoch_gen_loss/(batch_idx+1), disc_loss=epoch_disc_loss/(batch_idx+1), PSNR=f\"{epoch_psnr/(batch_idx+1):.4f}\")\n","\n","    # Append epoch metrics to the lists\n","    train_gen_losses.append(epoch_gen_loss / len(train_dataloader))\n","    train_disc_losses.append(epoch_disc_loss / len(train_dataloader))\n","    train_psnr_values.append(epoch_psnr / len(train_dataloader))\n","\n","    ### Testing (Validation)\n","    epoch_gen_loss, epoch_disc_loss = 0, 0\n","    epoch_psnr = 0\n","    tqdm_bar = tqdm(test_dataloader, desc=f'Testing Epoch {epoch} ', total=len(test_dataloader))\n","\n","    for batch_idx, imgs in enumerate(tqdm_bar):\n","        generator.eval()\n","        discriminator.eval()\n","\n","        # Configure model input\n","        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n","        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n","\n","        # Adversarial ground truths\n","        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n","        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n","\n","        ### Eval Generator\n","        # Generate a high resolution image from low resolution input\n","        gen_hr = generator(imgs_lr)\n","\n","        # Adversarial loss\n","        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n","        # Content loss\n","        gen_features = feature_extractor(gen_hr)\n","        real_features = feature_extractor(imgs_hr)\n","        loss_content = criterion_content(gen_features, real_features.detach())\n","        # Total loss\n","        loss_G = loss_content + 1e-3 * loss_GAN\n","\n","        ### Eval Discriminator\n","        # Loss of real and fake images\n","        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n","        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n","        # Total loss\n","        loss_D = (loss_real + loss_fake) / 2\n","\n","        # Accumulate losses\n","        epoch_gen_loss += loss_G.item()\n","        epoch_disc_loss += loss_D.item()\n","\n","        # Calculate PSNR\n","        imgs_hr_np = imgs_hr.cpu().detach().numpy()\n","        gen_hr_np = gen_hr.cpu().detach().numpy()\n","        psnr_value = PSNR(imgs_hr_np, gen_hr_np, data_range=1.0)\n","        epoch_psnr += psnr_value\n","\n","        tqdm_bar.set_postfix(gen_loss=epoch_gen_loss/(batch_idx+1), disc_loss=epoch_disc_loss/(batch_idx+1), PSNR=f\"{epoch_psnr/(batch_idx+1):.4f}\")\n","\n","    # Append epoch metrics to the lists\n","    test_gen_losses.append(epoch_gen_loss / len(test_dataloader))\n","    test_disc_losses.append(epoch_disc_loss / len(test_dataloader))\n","    test_psnr_values.append(epoch_psnr / len(test_dataloader))\n","\n","    # Save model checkpoints based on validation loss\n","    if np.argmin(test_gen_losses) == len(test_gen_losses) - 1:\n","        torch.save(generator.state_dict(), \"saved_models/generator.pth\")\n","        torch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOFUPfI3rd_x"},"outputs":[],"source":["import torch\n","from torch.autograd import Variable\n","from torchvision.utils import save_image\n","import matplotlib.pyplot as plt\n","\n","\n","generator = GeneratorVGG()\n","generator.load_state_dict(torch.load('saved_models/generator.pth'))\n","generator.eval()\n","\n","\n","\n","fig, axs = plt.subplots(4, 2, figsize=(10, 20))\n","\n","for idx in range(4):\n","    imgs = next(iter(test_dataloader))\n","\n","    imgs_lr = Variable(imgs[\"lr\"].type(torch.FloatTensor))\n","\n","    with torch.no_grad():\n","        gen_hr = generator(imgs_lr)\n","\n","    original_hr = imgs[\"hr\"][0].permute(1, 2, 0).numpy()\n","    generated_hr = gen_hr[0].cpu().permute(1, 2, 0).numpy()\n","\n","    # Plot original HR image\n","    axs[idx, 0].imshow(original_hr)\n","    axs[idx, 0].set_title('Original HR')\n","    axs[idx, 0].axis('off')\n","\n","    # Plot generated HR image\n","    axs[idx, 1].imshow(generated_hr)\n","    axs[idx, 1].set_title('Generated HR')\n","    axs[idx, 1].axis('off')\n","\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":29561,"sourceId":37705,"sourceType":"datasetVersion"},{"sourceId":51918360,"sourceType":"kernelVersion"}],"dockerImageVersionId":30017,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}